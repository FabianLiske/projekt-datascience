{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180ea84b-67b8-46a1-8d3e-af48835e9a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35702851-9ebd-44fa-a80d-b9be21d2a1cf",
   "metadata": {},
   "source": [
    "# Spalten Checken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7e1f346-eb20-4575-9c34-1a4fe5acaee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gefundene CSVs: 61\n"
     ]
    }
   ],
   "source": [
    "sf_raw = Path(\"/workspaces/projekt-datascience/data/raw/montreal\")\n",
    "csv_files = sorted(sf_raw.glob(\"**/*.csv*\"))\n",
    "print(f\"gefundene CSVs: {len(csv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4982edf6-c2d5-4603-b387-99201e5c52c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_columns_fastish(p):\n",
    "    cols = pd.read_csv(p, nrows=0).columns\n",
    "    raw_cols = list(cols)\n",
    "    norm_cols = tuple(col.strip().lower() for col in cols)\n",
    "    return raw_cols, norm_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f827224c-e975-480a-9f70-f1f6cdf3b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_groups = defaultdict(list)\n",
    "col_counter = Counter()\n",
    "per_file_cols = {}\n",
    "\n",
    "for f in csv_files:\n",
    "    raw_cols, norm_cols = read_columns_fastish(f)\n",
    "    schema_groups[norm_cols].append(f)\n",
    "    col_counter.update(norm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48408d89-0455-41bc-9566-e47f8e278a6a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einzigartige Schemas\n",
      "\n",
      "Schema #1 — 7 Datei(en)\n",
      "  • 2014/Stations_2014.csv\n",
      "  • 2015/Stations_2015.csv\n",
      "  • 2016/Stations_2016.csv\n",
      "  • 2017/Stations_2017.csv\n",
      "  • 2018/Stations_2018.csv\n",
      "  • 2019/Stations_2019.csv\n",
      "  • 2020/stations.csv\n",
      "Spalten:\n",
      "  - code\n",
      "  - name\n",
      "  - latitude\n",
      "  - longitude\n",
      "\n",
      "Schema #2 — 48 Datei(en)\n",
      "  • 2014/monthly/OD_2014-04.csv\n",
      "  • 2014/monthly/OD_2014-05.csv\n",
      "  • 2014/monthly/OD_2014-06.csv\n",
      "  • 2014/monthly/OD_2014-07.csv\n",
      "  • 2014/monthly/OD_2014-08.csv\n",
      "  • 2014/monthly/OD_2014-09.csv\n",
      "  • 2014/monthly/OD_2014-10.csv\n",
      "  • 2014/monthly/OD_2014-11.csv\n",
      "  • 2015/monthly/OD_2015-04.csv\n",
      "  • 2015/monthly/OD_2015-05.csv\n",
      "  • 2015/monthly/OD_2015-06.csv\n",
      "  • 2015/monthly/OD_2015-07.csv\n",
      "  • 2015/monthly/OD_2015-08.csv\n",
      "  • 2015/monthly/OD_2015-09.csv\n",
      "  • 2015/monthly/OD_2015-10.csv\n",
      "  • 2015/monthly/OD_2015-11.csv\n",
      "  • 2016/monthly/OD_2016-04.csv\n",
      "  • 2016/monthly/OD_2016-05.csv\n",
      "  • 2016/monthly/OD_2016-06.csv\n",
      "  • 2016/monthly/OD_2016-07.csv\n",
      "  • 2016/monthly/OD_2016-08.csv\n",
      "  • 2016/monthly/OD_2016-09.csv\n",
      "  • 2016/monthly/OD_2016-10.csv\n",
      "  • 2016/monthly/OD_2016-11.csv\n",
      "  • 2017/monthly/OD_2017-04.csv\n",
      "  • 2017/monthly/OD_2017-05.csv\n",
      "  • 2017/monthly/OD_2017-06.csv\n",
      "  • 2017/monthly/OD_2017-07.csv\n",
      "  • 2017/monthly/OD_2017-08.csv\n",
      "  • 2017/monthly/OD_2017-09.csv\n",
      "  • 2017/monthly/OD_2017-10.csv\n",
      "  • 2017/monthly/OD_2017-11.csv\n",
      "  • 2018/monthly/OD_2018-04.csv\n",
      "  • 2018/monthly/OD_2018-05.csv\n",
      "  • 2018/monthly/OD_2018-06.csv\n",
      "  • 2018/monthly/OD_2018-07.csv\n",
      "  • 2018/monthly/OD_2018-08.csv\n",
      "  • 2018/monthly/OD_2018-09.csv\n",
      "  • 2018/monthly/OD_2018-10.csv\n",
      "  • 2018/monthly/OD_2018-11.csv\n",
      "  • 2019/monthly/OD_2019-04.csv\n",
      "  • 2019/monthly/OD_2019-05.csv\n",
      "  • 2019/monthly/OD_2019-06.csv\n",
      "  • 2019/monthly/OD_2019-07.csv\n",
      "  • 2019/monthly/OD_2019-08.csv\n",
      "  • 2019/monthly/OD_2019-09.csv\n",
      "  • 2019/monthly/OD_2019-10.csv\n",
      "  • 2020/yearly/OD_2020.csv\n",
      "Spalten:\n",
      "  - start_date\n",
      "  - start_station_code\n",
      "  - end_date\n",
      "  - end_station_code\n",
      "  - duration_sec\n",
      "  - is_member\n",
      "\n",
      "Schema #3 — 1 Datei(en)\n",
      "  • 2021/2021_stations.csv\n",
      "Spalten:\n",
      "  - pk\n",
      "  - name\n",
      "  - latitude\n",
      "  - longitude\n",
      "\n",
      "Schema #4 — 1 Datei(en)\n",
      "  • 2021/yearly/2021_donnees_ouvertes.csv\n",
      "Spalten:\n",
      "  - start_date\n",
      "  - emplacement_pk_start\n",
      "  - end_date\n",
      "  - emplacement_pk_end\n",
      "  - duration_sec\n",
      "  - is_member\n",
      "\n",
      "Schema #5 — 4 Datei(en)\n",
      "  • 2022/yearly/DonneesOuverte2022.csv\n",
      "  • 2023/yearly/DonneesOuvertes (1).csv\n",
      "  • 2024/yearly/DonneesOuvertes (2).csv\n",
      "  • 2025/yearly/DonneesOuvertes2025_0102030405060708.csv\n",
      "Spalten:\n",
      "  - startstationname\n",
      "  - startstationarrondissement\n",
      "  - startstationlatitude\n",
      "  - startstationlongitude\n",
      "  - endstationname\n",
      "  - endstationarrondissement\n",
      "  - endstationlatitude\n",
      "  - endstationlongitude\n",
      "  - starttimems\n",
      "  - endtimems\n"
     ]
    }
   ],
   "source": [
    "print(\"Einzigartige Schemas\")\n",
    "for i, (schema, files) in enumerate(schema_groups.items(), start=1):\n",
    "    print(f\"\\nSchema #{i} — {len(files)} Datei(en)\")\n",
    "    for ex in files:\n",
    "        print(f\"  • {ex.relative_to(sf_raw)}\")\n",
    "    print(\"Spalten:\")\n",
    "    for c in schema:\n",
    "        print(f\"  - {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfa4be-7155-49e9-a3a6-c01ed16c5608",
   "metadata": {},
   "source": [
    "# San Fran in Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab51ad5-547e-40c0-a18a-bb7fd07dbf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e58ecd-00a7-489b-97eb-81cf1dd7a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_CITY = Path(\"/workspaces/projekt-datascience/data/raw/san_francisco\")\n",
    "OUT_BASE = \"/workspaces/projekt-datascience/data/parquet/bronze/usage\"  # Hive-Partitionen city/year/month\n",
    "CITY = \"san_francisco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070b7c65-d437-4c7f-b388-831e3b870f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbronze-parquet-bikes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.sources.partitionOverwriteMode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdynamic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.parquet.compression.codec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"bronze-parquet-bikes\")\n",
    "         .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "         .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "         .getOrCreate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
